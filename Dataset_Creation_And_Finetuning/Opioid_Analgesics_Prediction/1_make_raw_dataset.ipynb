{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import os\n",
    "import re\n",
    "from functools import partial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset_path = \"ALL_DATASETS\"\n",
    "os.makedirs(all_dataset_path, exist_ok=True)\n",
    "\n",
    "pre_final_df_csv_name = \"OPIOID_ANALGESIC_PRED_RACES_RAW.csv\"\n",
    "pre_final_df_csv_path = os.path.join(all_dataset_path, pre_final_df_csv_name)\n",
    "\n",
    "undersample_to_make_races_occur_uniformly = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = '/home/gokul/Hier-Legal-Graph/mimic_dataset/mimiciv_dataset/2.2/'\n",
    "\n",
    "opioid_analgesics = {\n",
    "    \"Morphine\": [\"Morphine\", \"MS Contin\", \"Kadian\", \"Avinza\", \"Roxanol\", \"Arymo ER\", \"Arymo\", \"Morphabond\", \"Oramorph SR\", \"Oramorph\", \"Duramorph\", \n",
    "                 \"Astramorph PF\", \"Astramorph\"],\n",
    "    \"Hydromorphone\": [\"Hydromorphone\", \"Dilaudid\", \"Exalgo\"],\n",
    "    \"Fentanyl\": [\"Fentanyl\", \"Duragesic\", \"Actiq\", \"Fentora\", \"Subsys\", \"Abstral\", \"Lazanda\", \"Sublimaze\"],\n",
    "    \"Oxycodone\": [\"Oxycodone\", \"OxyContin\", \"Roxicodone\", \"Xtampza ER\", \"Xtampza\", \"Percocet\", \"Percodan\", \"Targiniq ER\", \"Targiniq\"],\n",
    "    \"Hydrocodone\": [\"Hydrocodone\", \"Hysingla ER\", \"Hysingla\", \"Zohydro ER\", \"Zohydro\", \"Vicodin\", \"Norco\", \"Lortab\", \"Reprexain\"],\n",
    "    \"Codeine\": [\"Codeine\", \"Tylenol with Codeine\", \"Capital and Codeine\"],\n",
    "    \"Tramadol\": [\"Tramadol\", \"Ultram\", \"ConZip\", \"Ryzolt\"],\n",
    "    \"Methadone\": [\"Methadone\", \"Dolophine\", \"Methadose\"],\n",
    "    \"Oxymorphone\": [\"Oxymorphone\", \"Opana ER\", \"Opana\"],\n",
    "    \"Tapentadol\": [\"Tapentadol\", \"Nucynta\", \"Nucynta ER\"],\n",
    "    \"Meperidine\": [\"Meperidine\", \"Demerol\"],\n",
    "    \"Buprenorphine\": [\"Buprenorphine\", \"Subutex\", \"Butrans\", \"Belbuca\"],\n",
    "    \"Levorphanol\": [\"Levorphanol\", \"Levo-Dromoran\"],\n",
    "    \"Butorphanol\": [\"Butorphanol\", \"Stadol\"],\n",
    "    \"Propoxyphene\": [\"Propoxyphene\"],\n",
    "    \"Pentazocine\": [\"Pentazocine\", \"Talwin\", \"Talwin Nx\"],\n",
    "    \"Dihydrocodeine\": [\"Dihydrocodeine\"],\n",
    "    \"Nalbuphine\": [\"Nalbuphine\", \"Nubain\"],\n",
    "    \"Tilidine\": [\"Tilidine\"],\n",
    "    \"Dextropropoxyphene\": [\"Dextropropoxyphene\"],\n",
    "    \"Pethidine\": [\"Pethidine\", \"Demerol\"],\n",
    "    \"Oliceridine\": [\"Oliceridine\", \"Olinvyk\"],\n",
    "    \"Alfentanil\": [\"Alfentanil\", \"Alfenta\"],  \n",
    "    \"Remifentanil\": [\"Remifentanil\", \"Ultiva\"],  \n",
    "    \"Sufentanil\": [\"Sufentanil\", \"Dsuvia\", \"Sufenta\"] \n",
    "}\n",
    "\n",
    "opioid_analgesics_names_list = sorted({name.strip().lower() for names in opioid_analgesics.values() for name in names})\n",
    "\n",
    "\n",
    "def load_data(file_path: str, columns: list[str]=None, schema_overrides = None):\n",
    "    '''\n",
    "    file_path: Ignore '/home/gokul/Hier-Legal-Graph/mimic_dataset/mimiciv_dataset/2.2/' in the file path.\n",
    "    That will be taken care of internally.\n",
    "    \n",
    "    columns: A list of column names to load from the CSV.\n",
    "\n",
    "    schema_overrides: Explicitly specify the datatype of certain columns by inferring the type from the MIMIC IV v3.1 docs when reading the csv normally raises errors\n",
    "    '''\n",
    "    base_path = '/home/gokul/Hier-Legal-Graph/mimic_dataset/mimiciv_dataset/2.2/'\n",
    "    file_path = base_path + file_path\n",
    "\n",
    "    # Pass the columns list to read_csv\n",
    "    df = pl.read_csv(file_path, columns=columns, schema_overrides=schema_overrides)\n",
    "    return df\n",
    "\n",
    "\n",
    "def format_diagnoses(diag_list: list[str]) -> str:\n",
    "    '''\n",
    "    Convert the diagnoses column which is currently a list of strings into a string column. We do this by joining all the diagnosis in the list into one \n",
    "    string after numbering the diagnosis\n",
    "\n",
    "    [diag1, diag2, ...., diag5] ---> \"\\n1. diag1 \\n2. diag2\\n.......\\n5. diag5\"\n",
    "    '''\n",
    "    return \"\\n\".join(f\"{i+1}. {d}\" for i, d in enumerate(diag_list))\n",
    "\n",
    "def process_text_from_notes(text: str, section_pattern: re.Pattern, \n",
    "                            deid_pattern: re.Pattern, med_pattern: re.Pattern, \n",
    "                            target_sections: list, deid_dict: dict,\n",
    "                            separator_to_join_sections = \"\\n\\n---___---!@#%&*JOIN_SEPARATOR*&%#@!---___---\\n\\n\") -> dict:\n",
    "    \"\"\"\n",
    "    Extracts specific medical sections, de-identifies gender-related terms, and redacts medication names.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input medical text.\n",
    "    section_pattern (re.Pattern): Regex to extract target sections.\n",
    "    deid_pattern (re.Pattern): Regex to replace gender-related terms.\n",
    "    med_pattern (re.Pattern): Regex to redact medication names.\n",
    "    target_sections (list): List of target sections to be extracted from the notes\n",
    "    deid_dict (dict): A dictionary for gender de-identification.\n",
    "\n",
    "    Returns:\n",
    "    dict: Processed text and a flag (1 if all sections were found, else 0).\n",
    "    \"\"\"\n",
    "\n",
    "    matches = section_pattern.findall(text)\n",
    "\n",
    "    # Count found sections\n",
    "    found_sections = set()\n",
    "    for match in matches:\n",
    "        for section in target_sections:\n",
    "            if match.strip().startswith(section):\n",
    "                found_sections.add(section)\n",
    "                break\n",
    "    \n",
    "    # The flag is 1 if all target sections were extracted from the note.\n",
    "    section_flag = 1 if len(found_sections) == len(target_sections) else 0\n",
    "\n",
    "    \n",
    "    final_text = separator_to_join_sections.join(matches)\n",
    "\n",
    "    def replace(match):\n",
    "        # Lowercase the match and replace with the corresponding value from deid_dict\n",
    "        return deid_dict[match.group(0).lower()]\n",
    "\n",
    "    final_text = deid_pattern.sub(replace, final_text)\n",
    "\n",
    "    final_text = med_pattern.sub(\"___\", final_text)\n",
    "\n",
    "    return {\"processed_text_with_all_sections_combined\": final_text, \"notes_section_flag\": section_flag}\n",
    "\n",
    "def has_opioid(medicines, opioid_patterns):\n",
    "    for medicine in medicines:\n",
    "        for pattern in opioid_patterns:\n",
    "            if pattern.search(medicine):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def extract_section(text, section_title, separator_to_join_sections = \"\\n\\n---___---!@#%&*JOIN_SEPARATOR*&%#@!---___---\\n\\n\"):\n",
    "    # Split the text by the custom separator\n",
    "    sections = text.split(separator_to_join_sections)\n",
    "    \n",
    "    # Find the section that starts with the given title\n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if section.startswith(section_title):\n",
    "            # Remove the section title and colon, then return the cleaned content\n",
    "            content = section[len(section_title):].strip()\n",
    "            if content.startswith(':'):\n",
    "                content = content[1:].strip()\n",
    "            return content\n",
    "    \n",
    "    # Return empty string if section not found\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of the tables have been loaded\n",
      "Shape of main_df after loading admissions_df is: (180733, 16)\n",
      "\n",
      "==================\n",
      "Shape of main_df (after joining patients with admissions table) currently is: (180733, 21)\n",
      "\n",
      "==================\n",
      "Starting the ICD table code...\n",
      "The shape of the ICD codes + ICD Details joined is: (4756326, 6)\n",
      "This should have the same number of rows as icd_diagnosis_code_df at 4_756_326\n",
      "Shape of main_df after joining the ICD code and ICD Diagnosis Details: (180516, 23)\n",
      "\n",
      "==================\n",
      "Starting the Prescriptions table code...\n",
      "The ratios of UNK, NO, YES initially are:\n",
      "Fraction of UNK: 0.54\n",
      "Fraction of NO: 0.36\n",
      "Fraction of YES: 0.09\n",
      "Shape of main_df after including the prescription information is: (151054, 25)\n",
      "\n",
      "==================\n",
      "Starting the ICU Module code...\n",
      "The total number of unique hadm_id in the icu_stays table are: 66239\n",
      "Minimum stay duration: 0.00125\n",
      "Shape of main_df after adding the ICU_STAYS details is: (151054, 27)\n",
      "Starting the Notes Module code...\n",
      "Shape of main_df after just loading the notes module and appending the text column to main_df is: (134722, 28)\n",
      "\n",
      "==================\n",
      "Starting the processing of the notes to extract the sections that we want\n",
      "The sections that will be extracted are: \n",
      "Allergies\n",
      "Chief Complaint\n",
      "History of Present Illness\n",
      "Past Medical History\n",
      "\n",
      "\n",
      "Finished process the notes for the whole dataset, total time taken: 266.37 seconds\n",
      "Starting the gendered disease de-identification of the notes\n",
      "Finished the gendered disease de-identification of the notes, time taken: 1.66 seconds\n",
      "The shape of the dataset after processing and joining the notes module is: (134722, 34)\n",
      "\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "admissions_df = load_data('hosp/admissions.csv')\n",
    "patients_df = load_data('hosp/patients.csv')\n",
    "icd_diagnosis_code_details_df = load_data('hosp/d_icd_diagnoses.csv', schema_overrides = {\"icd_code\": pl.Utf8})\n",
    "icd_diagnosis_code_df = load_data('hosp/diagnoses_icd.csv')\n",
    "icustays_df = load_data('icu/icustays.csv')\n",
    "notes_df = load_data('mimic-iv-note-deidentified-free-text-clinical-notes-2.2/note/discharge.csv')\n",
    "\n",
    "print(f\"All of the tables have been loaded\")\n",
    "\n",
    "# Loading a subset of columns because polars is running into an error while inferring schema of \"gsn\" column. schema_overrides can be used but since we do not need gsn column, we do  not load it.\n",
    "prescriptions_df = load_data('hosp/prescriptions.csv', columns=['subject_id', 'hadm_id', 'pharmacy_id', 'drug'])\n",
    "\n",
    "# Now, in the admissions table, I only want to keep rows that belong to the last hadm_id of a particular subject_id.\n",
    "admissions_last_hadm_id_df = (admissions_df.group_by(\"subject_id\").agg(\n",
    "    pl.all().sort_by(\"admittime\", descending=True).first()\n",
    "    ))\n",
    "\n",
    "print(f\"Shape of main_df after loading admissions_df is: {admissions_last_hadm_id_df.shape}\\n\\n==================\")\n",
    "\n",
    "\n",
    "# Inner join tables of admission and patients --> we only want rows where both information is present.\n",
    "main_df = admissions_last_hadm_id_df.join(patients_df, on=\"subject_id\", how=\"inner\")\n",
    "print(f\"Shape of main_df (after joining patients with admissions table) currently is: {main_df.shape}\\n\\n==================\")\n",
    "\n",
    "################\n",
    "### ICD CODE ###\n",
    "################\n",
    "print(f\"Starting the ICD table code...\")\n",
    "\n",
    "# Joining the ICD code details to the ICD Diagnosis Codes table\n",
    "icd_diagnosis_code_with_details_joined_df = icd_diagnosis_code_df.join(\n",
    "    icd_diagnosis_code_details_df, \n",
    "    on=[\"icd_version\", \"icd_code\"], \n",
    "    how=\"inner\"  # Use inner join to retain records with all required information\n",
    ")\n",
    "\n",
    "print(f\"The shape of the ICD codes + ICD Details joined is: {icd_diagnosis_code_with_details_joined_df.shape}\\nThis should have the same number of rows as icd_diagnosis_code_df at 4_756_326\")\n",
    "\n",
    "icd_diagnosis_code_with_details_joined_and_grouped_by_hadm_id_df = icd_diagnosis_code_with_details_joined_df.group_by(\"hadm_id\").agg(\n",
    "    pl.col(\"long_title\").alias(\"long_title_list\")\n",
    ")\n",
    "\n",
    "# Now take the list[str] column and make a new column with just strings (converts it into the format defined inside format_diagnosis function)\n",
    "icd_diagnosis_code_with_details_joined_and_grouped_by_hadm_id_df = icd_diagnosis_code_with_details_joined_and_grouped_by_hadm_id_df.with_columns(\n",
    "    pl.col(\"long_title_list\")\n",
    "      .map_elements(format_diagnoses, return_dtype=pl.Utf8)\n",
    "      .alias(\"numbered_diagnoses\")\n",
    ")\n",
    "\n",
    "# Inner join tables of main_df and icd_diagnosis_code_with_details_joined_df --> we only want rows where both information is present. (There are some hadm_ids in main_df that do NOT have ICD codes)\n",
    "main_df = main_df.join(icd_diagnosis_code_with_details_joined_and_grouped_by_hadm_id_df, on=\"hadm_id\", how=\"inner\")\n",
    "print(f\"Shape of main_df after joining the ICD code and ICD Diagnosis Details: {main_df.shape}\\n\\n==================\")\n",
    "\n",
    "###########################\n",
    "### Prescriptions Table ###\n",
    "###########################\n",
    "print(f\"Starting the Prescriptions table code...\")\n",
    "\n",
    "prescriptions_df = prescriptions_df.with_columns(pl.col(\"drug\").str.to_lowercase().str.strip_chars())\n",
    "\n",
    "# Group by hadm_id and aggregate all drugs given to the subject in that hadm_id into a list[str]\n",
    "prescriptions_group_by_hadm_id_df = prescriptions_df.group_by(\"hadm_id\").agg(pl.col(\"drug\").alias(\"list_of_administered_meds\"))\n",
    "\n",
    "#### NOW WE MAKE THE GT_FLAG column. ####\n",
    "# intialise the column as empty strings.\n",
    "prescriptions_group_by_hadm_id_df = prescriptions_group_by_hadm_id_df.with_columns(pl.lit(value=\"\", dtype=pl.Utf8).alias(\"GT_FLAG\"))\n",
    "\n",
    "# Fill GT_FLAG with \"YES\"\n",
    "prescriptions_group_by_hadm_id_df = prescriptions_group_by_hadm_id_df.with_columns(\n",
    "    pl.when(\n",
    "        pl.col(\"list_of_administered_meds\")\n",
    "          .list.eval(pl.element().is_in(opioid_analgesics_names_list))\n",
    "          .list.any()  # Returns True if any administered med is in the opioid list\n",
    "    )\n",
    "    .then(pl.lit(\"YES\"))\n",
    "    .otherwise(pl.col(\"GT_FLAG\"))\n",
    "    .alias(\"GT_FLAG\")\n",
    ")\n",
    "\n",
    "#### Fill GT_FLAG with \"UNK\"\n",
    "# Compile regex patterns for all opioids \n",
    "opioid_patterns = [re.compile(opioid, re.IGNORECASE) for opioid in opioid_analgesics_names_list]\n",
    "\n",
    "has_opioid_partial = partial(has_opioid, \n",
    "                               opioid_patterns=opioid_patterns)\n",
    "\n",
    "prescriptions_group_by_hadm_id_df = prescriptions_group_by_hadm_id_df.with_columns([\n",
    "    pl.when(\n",
    "        (pl.col(\"GT_FLAG\") != \"YES\") & \n",
    "        pl.col(\"list_of_administered_meds\").map_elements(has_opioid_partial, return_dtype=pl.Boolean)\n",
    "    ).then(pl.lit(value=\"UNK\", dtype=pl.Utf8)).otherwise(pl.col(\"GT_FLAG\")).alias(\"GT_FLAG\")\n",
    "])\n",
    "\n",
    "#### Fill all remaining rows with not GT_FLAG value as \"NO\"\n",
    "prescriptions_group_by_hadm_id_df = prescriptions_group_by_hadm_id_df.with_columns(\n",
    "    pl.when(pl.col(\"GT_FLAG\") == \"\")\n",
    "    .then(pl.lit(\"NO\"))\n",
    "    .otherwise(pl.col(\"GT_FLAG\")).alias(\"GT_FLAG\")\n",
    ")\n",
    "\n",
    "print(f\"The ratios of UNK, NO, YES initially are:\")\n",
    "print(f'Fraction of UNK: {len(prescriptions_group_by_hadm_id_df.filter(pl.col(\"GT_FLAG\") == \"UNK\")) / len(prescriptions_group_by_hadm_id_df):.2f}')\n",
    "print(f'Fraction of NO: {len(prescriptions_group_by_hadm_id_df.filter(pl.col(\"GT_FLAG\") == \"NO\")) / len(prescriptions_group_by_hadm_id_df):.2f}')\n",
    "print(f'Fraction of YES: {len(prescriptions_group_by_hadm_id_df.filter(pl.col(\"GT_FLAG\") == \"YES\")) / len(prescriptions_group_by_hadm_id_df):.2f}')\n",
    "\n",
    "# Inner join tables of main_df and prescriptions_group_by_hadm_id_df --> we only want rows where both information is present.\n",
    "main_df = main_df.join(prescriptions_group_by_hadm_id_df, on=\"hadm_id\", how=\"inner\")\n",
    "print(f\"Shape of main_df after including the prescription information is: {main_df.shape}\\n\\n==================\")\n",
    "\n",
    "##################\n",
    "### ICU Module ###\n",
    "##################\n",
    "print(f\"Starting the ICU Module code...\")\n",
    "\n",
    "print(f\"The total number of unique hadm_id in the icu_stays table are: {icustays_df['hadm_id'].n_unique()}\")\n",
    "\n",
    "# For each hadm_id, take the sum of the length of stay in ICU for each stay_id of a particular hadm_id --> proxy for how serious the condition was if the patient did not die in the ICU\n",
    "icustays_group_by_hadm_sum_of_los_df = icustays_df.group_by(\"hadm_id\").agg(pl.col(\"los\").sum().alias(\"Total_LOS_in_ICU_in_days\"))\n",
    "\n",
    "# Minimum value of Total_LOS_in_ICU_in_days for someone admitted to the ICU is >0, therefore, we can replace NULL values with 0 \n",
    "min_stay_duration = icustays_group_by_hadm_sum_of_los_df.select(\"Total_LOS_in_ICU_in_days\").min()[\"Total_LOS_in_ICU_in_days\"][0]\n",
    "print(f\"Minimum stay duration: {min_stay_duration}\")\n",
    "\n",
    "\n",
    "\n",
    "# Now, we create two columns, one columns is --> \"was_admitted_to_icu\" (This will be 1 for a given hadm_id, if the id is present in icustays_group_by_hadm_sum_of_los_df, and 0 otherwise)\n",
    "# other column is Total_LOS_in_ICU_in_days (This will be 0 if hadm_id was not present in icustays_group_by_hadm_sum_of_los_df, or the value of Total_LOS_in_ICU_in_days if it was present)\n",
    "# Left join main_df with ICU LOS DataFrame on hadm_id. The hadm_id that were not present in the icustays_group_by_hadm_sum_of_los_df will have value as NULL for this column\n",
    "main_df = main_df.join(icustays_group_by_hadm_sum_of_los_df, on=\"hadm_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# Fill missing Total_LOS_in_ICU_in_days with 0\n",
    "main_df = main_df.with_columns(\n",
    "    pl.col(\"Total_LOS_in_ICU_in_days\").fill_null(0)\n",
    ")\n",
    "\n",
    "# Create was_admitted_to_icu: 1 if Total_LOS_in_ICU_in_days > 0, else 0 {since minimum value of someone admitted to the ICU was >0, 0.00125 to be precise}\n",
    "main_df = main_df.with_columns(\n",
    "    (pl.col(\"Total_LOS_in_ICU_in_days\") > 0).cast(pl.Int8).alias(\"was_admitted_to_icu\")\n",
    ")\n",
    "\n",
    "print(f\"Shape of main_df after adding the ICU_STAYS details is: {main_df.shape}\")\n",
    "\n",
    "####################\n",
    "### Notes Module ###\n",
    "####################\n",
    "print(f\"Starting the Notes Module code...\")\n",
    "\n",
    "main_df = main_df.join(notes_df[[\"hadm_id\", \"text\"]], on=\"hadm_id\", how=\"inner\")\n",
    "print(f\"Shape of main_df after just loading the notes module and appending the text column to main_df is: {main_df.shape}\\n\\n==================\")\n",
    "\n",
    "# Select the notes of the hadm_ids that are present in the main_df table.\n",
    "notes_last_hadm_id_df = main_df[[\"hadm_id\", \"text\"]]\n",
    "\n",
    "print(f\"Starting the processing of the notes to extract the sections that we want\")\n",
    "time1 = time.time()\n",
    "\n",
    "target_sections = [\n",
    "    'Allergies',\n",
    "    'Chief Complaint',\n",
    "    'History of Present Illness',\n",
    "    'Past Medical History'\n",
    "]\n",
    "\n",
    "print(f\"The sections that will be extracted are: \")\n",
    "print(*target_sections, sep=\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "pattern = (\n",
    "    r'\\n\\s*\\n('                                   # Match preceding blank line(s) then start capturing group for the section\n",
    "    r'(?:' + '|'.join(re.escape(section) for section in target_sections) + r')'  # Match one of the target section titles\n",
    "    r'\\s*:'                                       # Optional whitespace and the colon after the title\n",
    "    r'.*?'                                        # Lazily match all content (including newlines)\n",
    "    r')'                                          # End capturing group\n",
    "    r'(?=\\n\\s*\\n[A-Z][^\\n]*?:|\\Z)'                 # Lookahead: next heading (blank line then a capital letter line ending with colon) or end of string\n",
    ")\n",
    "\n",
    "section_pattern = re.compile(pattern, re.DOTALL)\n",
    "\n",
    "# Now perform gender de-identification on the final_text using the dictionary.\n",
    "deid_dict = {\n",
    "    'male': 'person',\n",
    "    'female': 'person',\n",
    "    'he': 'the patient',\n",
    "    'she': 'the patient',\n",
    "    'him': 'the patient',\n",
    "    'her': \"the patient's\",\n",
    "    'his': \"the patient's\",\n",
    "    'hers': \"the patient's\",\n",
    "    'himself': 'the patient',\n",
    "    'herself': 'the patient',\n",
    "    'mr': 'the patient',\n",
    "    'mrs': 'the patient',\n",
    "    'ms': 'the patient',\n",
    "    'miss': 'the patient',\n",
    "    'mister': 'the patient',\n",
    "    'sir': 'the patient',\n",
    "    'madam': 'the patient',\n",
    "    'man': 'person',\n",
    "    'woman': 'person',\n",
    "    'men': 'people',\n",
    "    'women': 'people',\n",
    "    'gentleman': 'person',\n",
    "    'gentlewoman': 'person',\n",
    "    'boy': 'child',\n",
    "    'girl': 'child',\n",
    "    'boys': 'children',\n",
    "    'girls': 'children',\n",
    "    'father': 'parent',\n",
    "    'mother': 'parent',\n",
    "    'dad': 'parent',\n",
    "    'mom': 'parent',\n",
    "    'son': 'child',\n",
    "    'daughter': 'child',\n",
    "    'brother': 'sibling',\n",
    "    'sister': 'sibling',\n",
    "    'husband': 'spouse',\n",
    "    'wife': 'spouse',\n",
    "    'uncle': 'relative',\n",
    "    'aunt': 'relative'\n",
    "}\n",
    "\n",
    "# Build a regex to match any key from the de-identification dictionary as a whole word (case-insensitive)\n",
    "deid_pattern = re.compile(r'\\b(?:' + '|'.join(map(re.escape, deid_dict.keys())) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Build a regex to match any medication name (as a whole word/phrase, case-insensitive)\n",
    "med_pattern = re.compile('(?:' + '|'.join(map(re.escape, opioid_analgesics_names_list)) + ')', re.IGNORECASE)\n",
    "\n",
    "process_text_partial = partial(process_text_from_notes, \n",
    "                               section_pattern=section_pattern, \n",
    "                               deid_pattern=deid_pattern, \n",
    "                               med_pattern=med_pattern, \n",
    "                               target_sections=target_sections, \n",
    "                               deid_dict=deid_dict)\n",
    "\n",
    "# Apply the processing function to each row\n",
    "notes_last_hadm_id_df = notes_last_hadm_id_df.with_columns(\n",
    "    pl.col(\"text\").map_elements(\n",
    "        process_text_partial,\n",
    "        return_dtype=pl.Struct([\n",
    "            pl.Field(\"processed_text_with_all_sections_combined\", pl.Utf8),\n",
    "            pl.Field(\"notes_section_flag\", pl.Int64)\n",
    "        ])\n",
    "    ).alias(\"processed\")\n",
    ").unnest(\"processed\")\n",
    "\n",
    "print(f\"Finished process the notes for the whole dataset, total time taken: {time.time() - time1:.2f} seconds\")\n",
    "\n",
    "############# FOR A MORE COMPREHENSIVE LIST, LOOK AT ORIGINAL CODE FROM compile_dataset_new.ipynb in the original Med-LLM-Fairness repo\n",
    "male_indicators = ['prostate',\n",
    " 'testicular',\n",
    " 'penis',\n",
    " 'scrotum',\n",
    " 'spermatic',\n",
    " 'testis',\n",
    " 'epididymis',\n",
    " 'phallus',\n",
    " 'penile',\n",
    " 'deferens',\n",
    " 'ejaculate',\n",
    " 'ejaculation',\n",
    " 'sperm',\n",
    " 'prostatic',\n",
    " 'andropause',\n",
    " 'smegma',\n",
    " 'azoospermia',\n",
    " 'cryptorchidism',\n",
    " 'varicocele',\n",
    " 'spermatogenesis',\n",
    " 'paternity',\n",
    " 'spermatorrhea',\n",
    " 'Leydig',\n",
    " 'Sertoli',\n",
    " 'orchidometry',\n",
    " 'impotence',\n",
    " 'gynecomastia',\n",
    " 'spermatogenic',\n",
    " 'hypogonadism',\n",
    " 'semen',\n",
    " 'andrology',\n",
    " 'spermiogram',\n",
    " 'vasectomy',\n",
    " 'orchiectomy',\n",
    " 'orchidopexy',\n",
    " 'penectomy',\n",
    " 'circumcision',\n",
    " 'prostatectomy',\n",
    " 'scrotoplasty',\n",
    " 'penoplasty',\n",
    " 'varicocelectomy',\n",
    " 'phalloplasty',\n",
    " 'prostatitis',\n",
    " 'orchitis',\n",
    " 'epididymitis',\n",
    " 'spermatocele',\n",
    " 'chordee',\n",
    " 'phimosis',\n",
    " 'paraphimosis',\n",
    " 'balanitis',\n",
    " 'orchalgia',\n",
    " 'seminal',\n",
    " 'scrotal',\n",
    " 'penoscrotal',\n",
    " 'azoospermic',\n",
    " 'oligospermia',\n",
    " 'epididymal',\n",
    " 'orchidectomy',\n",
    " 'seminoma',\n",
    " 'hematospermia',\n",
    " 'balanoposthitis',\n",
    " 'Peyronie',\n",
    " 'Klinefelter']\n",
    "\n",
    "female_indicators = ['ovarian',\n",
    " 'uterine',\n",
    " 'vaginal',\n",
    " 'cervical',\n",
    " 'breast',\n",
    " 'myomectomy',\n",
    " 'fallopian',\n",
    " 'mammary',\n",
    " 'vulvar',\n",
    " 'clitoral',\n",
    " 'labial',\n",
    " 'endometrial',\n",
    " 'cervix',\n",
    " 'ovulation',\n",
    " 'uterus',\n",
    " 'vagina',\n",
    " 'PCOS',\n",
    " 'hysteroscopy',\n",
    " 'hysterosalpingogram',\n",
    " 'endometritis',\n",
    " 'salpingography'\n",
    " 'vulva',\n",
    " 'adnexal',\n",
    " 'areolar',\n",
    " 'pregnant',\n",
    " 'menstruation',\n",
    " 'gravida',\n",
    " 'parity',\n",
    " 'menstrual',\n",
    " 'amenorrhea',\n",
    " 'menarche',\n",
    " 'menopause',\n",
    " 'lactation',\n",
    " 'postpartum',\n",
    " 'antenatal',\n",
    " 'obstetric',\n",
    " 'gestational',\n",
    " 'preeclampsia',\n",
    " 'eclampsia',\n",
    " 'hydatidiform',\n",
    " 'fetal',\n",
    " 'luteal',\n",
    " 'follicular',\n",
    " 'chorioamnionitis',\n",
    " 'miscarriage',\n",
    " 'abortion',\n",
    " 'mastitis',\n",
    " 'vulvodynia',\n",
    " 'cystocele',\n",
    " 'rectocele',\n",
    " 'anovulation',\n",
    " 'adenomyosis',\n",
    " 'oophoritis',\n",
    " 'mammoplasty',\n",
    " 'endometrioid',\n",
    " 'menometrorrhagia',\n",
    " 'gravidity',\n",
    " 'ovulatory',\n",
    " 'placenta',\n",
    " 'amniotic',\n",
    " 'placental',\n",
    " 'chorionic',\n",
    " 'hysterectomy',\n",
    " 'oophorectomy',\n",
    " 'salpingo-oophorectomy',\n",
    " 'pap smear',\n",
    " 'papanicolaou'\n",
    " 'mammography',\n",
    " 'colposcopy',\n",
    " 'cesarean',\n",
    " 'episiotomy',\n",
    " 'curettage',\n",
    " 'tubal',\n",
    " 'endometriosis',\n",
    " 'fibroid',\n",
    " 'polycystic',\n",
    " 'dysmenorrhea',\n",
    " 'menorrhagia',\n",
    " 'mastalgia',\n",
    " 'galactorrhea',\n",
    " 'fibrocystic',\n",
    " 'cervicitis',\n",
    " 'vulvitis',\n",
    " 'vaginismus']\n",
    "\n",
    "complete_list = male_indicators + female_indicators\n",
    "\n",
    "print(f\"Starting the gendered disease de-identification of the notes\")\n",
    "time1 = time.time()\n",
    "\n",
    "pattern = re.compile(r'\\b(?:' + '|'.join(map(re.escape, complete_list)) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "notes_last_hadm_id_df = notes_last_hadm_id_df.with_columns(\n",
    "    pl.col(\"processed_text_with_all_sections_combined\")\n",
    "    .str.contains_any(complete_list)\n",
    "    .cast(pl.Int64)  # Convert boolean to integer (1 for True, 0 for False)\n",
    "    .alias(\"to_discard\")\n",
    ")\n",
    "\n",
    "print(f\"Finished the gendered disease de-identification of the notes, time taken: {time.time() - time1:.2f} seconds\")\n",
    "\n",
    "# Now create four new columns for each section\n",
    "notes_last_hadm_id_df = notes_last_hadm_id_df.with_columns([\n",
    "    pl.col(\"processed_text_with_all_sections_combined\").map_elements(\n",
    "        lambda text: extract_section(text, \"Allergies\"),\n",
    "        return_dtype=pl.Utf8\n",
    "    ).alias(\"patient_Allergies\"),\n",
    "    \n",
    "    pl.col(\"processed_text_with_all_sections_combined\").map_elements(\n",
    "        lambda text: extract_section(text, \"Chief Complaint\"),\n",
    "        return_dtype=pl.Utf8\n",
    "    ).alias(\"patient_chief_complaint\"),\n",
    "    \n",
    "    pl.col(\"processed_text_with_all_sections_combined\").map_elements(\n",
    "        lambda text: extract_section(text, \"History of Present Illness\"),\n",
    "        return_dtype=pl.Utf8\n",
    "    ).alias(\"patient_history_of_present_illness\"),\n",
    "    \n",
    "    pl.col(\"processed_text_with_all_sections_combined\").map_elements(\n",
    "        lambda text: extract_section(text, \"Past Medical History\"),\n",
    "        return_dtype=pl.Utf8\n",
    "    ).alias(\"patient_past_medical_history\")\n",
    "])\n",
    "\n",
    "main_df = main_df.join(\n",
    "    notes_last_hadm_id_df[['hadm_id', 'notes_section_flag', 'patient_Allergies', 'patient_chief_complaint', 'patient_history_of_present_illness', 'patient_past_medical_history', 'to_discard']], \n",
    "    on=\"hadm_id\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(f\"The shape of the dataset after processing and joining the notes module is: {main_df.shape}\\n\\n==================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Pre Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset BEFORE dropping rows that are not present in the top 4 races: (121428, 34)\n",
      "Length of dataset AFTER dropping rows that are not present in the top 4 races: (120246, 34)\n",
      "Length of dataset AFTER dropping rows that had 'to_discard' flag = 1: (93212, 34)\n",
      "Length of dataset AFTER dropping rows that had 'GT_FLAG' flag = UNK: (44517, 34)\n"
     ]
    }
   ],
   "source": [
    "main_df = main_df.with_columns(\n",
    "    pl.col(\"race\").str.split(\"-\").list.first().str.strip_chars().str.split(\"/\").list.first().str.strip_chars().alias(\"race\")\n",
    ")\n",
    "\n",
    "# Drop races like \"Unknown\", \"Other\"\n",
    "main_df = main_df.filter(pl.col('race') != 'UNKNOWN')\n",
    "main_df = main_df.filter(pl.col('race') != 'OTHER')\n",
    "main_df = main_df.filter(pl.col('race') != 'UNABLE TO OBTAIN')\n",
    "main_df = main_df.filter(pl.col('race') != 'PATIENT DECLINED TO ANSWER')\n",
    "\n",
    "main_df = main_df.with_columns(\n",
    "    pl.col(\"race\").replace({\"HISPANIC\": \"HISPANIC/LATINO\", \"HISPANIC OR LATINO\": \"HISPANIC/LATINO\"})\n",
    ")\n",
    "\n",
    "race_values = list(main_df['race'].value_counts(sort=True).head(4)['race'])\n",
    "gender_values = list(main_df['gender'].unique())\n",
    "\n",
    "demographic_dict = {\n",
    "    'gender': gender_values,\n",
    "    'race': race_values\n",
    "}\n",
    "\n",
    "print(f\"Length of dataset BEFORE dropping rows that are not present in the top 4 races: {main_df.shape}\")\n",
    "\n",
    "main_df = main_df.filter(pl.col('race').is_in(demographic_dict['race']))\n",
    "\n",
    "print(f\"Length of dataset AFTER dropping rows that are not present in the top 4 races: {main_df.shape}\")\n",
    "\n",
    "\n",
    "#### WE STILL HAVENT DISCARDED THE ROWS WITH 'to_discard' flage = 1\n",
    "main_df = main_df.filter(pl.col('to_discard') == 0)\n",
    "\n",
    "print(f\"Length of dataset AFTER dropping rows that had 'to_discard' flag = 1: {main_df.shape}\")\n",
    "\n",
    "main_df = main_df.filter(\n",
    "    pl.col('GT_FLAG') != 'UNK'\n",
    ")\n",
    "\n",
    "print(f\"Length of dataset AFTER dropping rows that had 'GT_FLAG' flag = UNK: {main_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value counts of the races before undersamplign is: shape: (4, 2)\n",
      "┌─────────────────┬───────┐\n",
      "│ race            ┆ count │\n",
      "│ ---             ┆ ---   │\n",
      "│ str             ┆ u32   │\n",
      "╞═════════════════╪═══════╡\n",
      "│ HISPANIC/LATINO ┆ 2128  │\n",
      "│ ASIAN           ┆ 2056  │\n",
      "│ BLACK           ┆ 5871  │\n",
      "│ WHITE           ┆ 34462 │\n",
      "└─────────────────┴───────┘\n",
      "Fractions BEFORE undersampling the dataset such that all races occur at uniform intervals is:\n",
      "Fraction of NO: 0.78\n",
      "Fraction of YES: 0.22\n"
     ]
    }
   ],
   "source": [
    "print(f\"The value counts of the races before undersamplign is: {main_df['race'].value_counts()}\")\n",
    "\n",
    "print(\"Fractions BEFORE undersampling the dataset such that all races occur at uniform intervals is:\")\n",
    "print(f'Fraction of NO: {len(main_df.filter(pl.col(\"GT_FLAG\") == \"NO\")) / len(main_df):.2f}')\n",
    "print(f'Fraction of YES: {len(main_df.filter(pl.col(\"GT_FLAG\") == \"YES\")) / len(main_df):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main_df will now be sampled so that all the races occur at a uniform rate\n",
      "\n",
      "\n",
      "The shape of main_df is now: (8224, 34)\n",
      "\n",
      "\n",
      "Fractions AFTER undersampling the dataset such that all races occur at uniform intervals is:\n",
      "Fraction of NO: 0.82\n",
      "Fraction of YES: 0.18\n",
      "Length of dataset after removing rows with number of ICD codes >= 15: 6138\n",
      "\n",
      "\n",
      "Fractions AFTER removing rows with num_icd_codes >= 15:\n",
      "Fraction of NO: 0.82\n",
      "Fraction of YES: 0.18\n"
     ]
    }
   ],
   "source": [
    "if undersample_to_make_races_occur_uniformly:\n",
    "    print(f\"The main_df will now be sampled so that all the races occur at a uniform rate\")\n",
    "    # Under sample the dataframe and have the races occuring at uniform intervals\n",
    "    min_count = main_df['race'].value_counts(sort=True)[-1][0, 1]\n",
    "\n",
    "    sampled_df = pl.DataFrame()\n",
    "\n",
    "    for race in demographic_dict['race']:\n",
    "        tmp_df = main_df.filter(pl.col('race') == race)\n",
    "        if tmp_df.shape[0] > min_count:\n",
    "            tmp_df = tmp_df.sample(n = min_count, with_replacement=False, seed=42)\n",
    "        sampled_df = pl.concat([sampled_df, tmp_df])\n",
    "\n",
    "    main_df_sampled = sampled_df\n",
    "else:\n",
    "    print(f\"The main_df will NOT be sampled.\")\n",
    "    main_df_sampled = main_df\n",
    "\n",
    "\n",
    "print(f'\\n\\nThe shape of main_df is now: {main_df_sampled.shape}\\n\\n')\n",
    "\n",
    "print(\"Fractions AFTER undersampling the dataset such that all races occur at uniform intervals is:\")\n",
    "print(f'Fraction of NO: {len(main_df_sampled.filter(pl.col(\"GT_FLAG\") == \"NO\")) / len(main_df_sampled):.2f}')\n",
    "print(f'Fraction of YES: {len(main_df_sampled.filter(pl.col(\"GT_FLAG\") == \"YES\")) / len(main_df_sampled):.2f}')\n",
    "\n",
    "main_df_sampled = main_df_sampled.with_columns(\n",
    "    num_icd_codes = pl.col(\"long_title_list\").list.len().alias('num_icd_codes')\n",
    ")\n",
    "\n",
    "icd_threshold = 15\n",
    "main_df_sampled_and_gender_based_disease_rows_discarded_and_limit_on_icd_codes_UNK_dropped = main_df_sampled.filter(pl.col('num_icd_codes') < icd_threshold)\n",
    "print(f'Length of dataset after removing rows with number of ICD codes >= {icd_threshold}: {main_df_sampled_and_gender_based_disease_rows_discarded_and_limit_on_icd_codes_UNK_dropped.shape[0]}\\n\\n')\n",
    "\n",
    "print(f\"Fractions AFTER removing rows with num_icd_codes >= {icd_threshold}:\")\n",
    "print(f'Fraction of NO: {len(main_df_sampled.filter(pl.col(\"GT_FLAG\") == \"NO\")) / len(main_df_sampled):.2f}')\n",
    "print(f'Fraction of YES: {len(main_df_sampled.filter(pl.col(\"GT_FLAG\") == \"YES\")) / len(main_df_sampled):.2f}')\n",
    "\n",
    "\n",
    "# Selecting relevant rows from the final dataframe to make the pre_final_df (the dataframe using which, prompts will be made according to the prompt template)\n",
    "pre_final_df = main_df_sampled_and_gender_based_disease_rows_discarded_and_limit_on_icd_codes_UNK_dropped.select(\n",
    "    cs.by_name(\n",
    "        \"subject_id\",\n",
    "        \"hadm_id\",\n",
    "        \"gender\",\n",
    "        \"race\",\n",
    "        \"anchor_age\",\n",
    "        \"patient_chief_complaint\",\n",
    "        \"patient_Allergies\",\n",
    "        \"patient_history_of_present_illness\",\n",
    "        \"patient_past_medical_history\",\n",
    "        \"numbered_diagnoses\",\n",
    "        \"was_admitted_to_icu\",\n",
    "        \"Total_LOS_in_ICU_in_days\",\n",
    "        \"GT_FLAG\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_final_df.write_csv(pre_final_df_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal_graph2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
