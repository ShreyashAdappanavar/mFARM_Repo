{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = '/home/gokul/Hier-Legal-Graph/mimic_dataset/mimiciv_dataset/2.2/'\n",
    "max_seq_num = 5\n",
    "\n",
    "all_dataset_path = \"ALL_DATASETS\"\n",
    "os.makedirs(all_dataset_path, exist_ok=True)\n",
    "\n",
    "pre_final_df_csv_name = \"ED_TRIAGE_PREDICTION_RAW.csv\"\n",
    "pre_final_df_csv_path = os.path.join(all_dataset_path, pre_final_df_csv_name)\n",
    "\n",
    "def load_data(file_path: str, columns: list[str]=None, schema_overrides = None):\n",
    "    '''\n",
    "    file_path: Ignore '/home/gokul/Hier-Legal-Graph/mimic_dataset/mimiciv_dataset/2.2/' in the file path.\n",
    "    That will be taken care of internally.\n",
    "    \n",
    "    columns: A list of column names to load from the CSV.\n",
    "\n",
    "    schema_overrides: Explicitly specify the datatype of certain columns by inferring the type from the MIMIC IV v3.1 docs when reading the csv normally raises errors\n",
    "    '''\n",
    "    base_path = '/home/gokul/Hier-Legal-Graph/mimic_dataset/mimiciv_dataset/2.2/'\n",
    "    file_path = base_path + file_path\n",
    "\n",
    "    # Pass the columns list to read_csv\n",
    "    df = pl.read_csv(file_path, columns=columns, schema_overrides=schema_overrides)\n",
    "    return df\n",
    "\n",
    "\n",
    "def format_diagnoses(diag_list: list[str]) -> str:\n",
    "    '''\n",
    "    Convert the diagnoses column which is currently a list of strings into a string column. We do this by joining all the diagnosis in the list into one \n",
    "    string after numbering the diagnosis\n",
    "\n",
    "    [diag1, diag2, ...., diag5] ---> \"\\n1. diag1 \\n2. diag2\\n.......\\n5. diag5\"\n",
    "    '''\n",
    "    return \"\\n\".join(f\"{i+1}. {d}\" for i, d in enumerate(diag_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ED_Diag shape (length) BEFORE filtering rows with more than 5 diagnosis: 949172\n",
      "ED_Diag shape (length) AFTER filtering rows with more than 5 diagnosis: 912855\n",
      "Shape of main_df BEFORE dropping non-alphanumeric ChiefComplaint: 379468\n",
      "Shape of main_df AFTER dropping non-alphanumeric ChiefComplaint: 332438\n",
      "\n",
      "Sum of rows with acuity 1.0 or 2.0: 101445\n",
      "Sum of rows with acuity 3.0: 134903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edstays = load_data(\"ed/edstays.csv\")\n",
    "ed_diagnosis = load_data(\"ed/diagnosis.csv\")\n",
    "# medrecon = load_data(\"ed/medrecon.csv\")\n",
    "triage = load_data(\"ed/triage.csv\")\n",
    "# vitalsign = load_data(\"ed/vitalsign.csv\")\n",
    "admissions = load_data(\"hosp/admissions.csv\")\n",
    "patients_df = load_data('hosp/patients.csv')\n",
    "\n",
    "main_df = edstays.join(patients_df, on=\"subject_id\", how=\"inner\")\n",
    "main_df = main_df.drop([\"anchor_year\", \"anchor_year_group\"])\n",
    "\n",
    "triage = triage.drop_nulls(subset=[\"temperature\", \"heartrate\", \"resprate\", \"o2sat\", \"sbp\", \"dbp\", \"pain\", \"acuity\", \"chiefcomplaint\"])\n",
    "main_df = main_df.join(triage, on=\"stay_id\", how=\"inner\")\n",
    "main_df = main_df.drop(\"subject_id_right\")\n",
    "\n",
    "ed_diagnosis = ed_diagnosis.sort([\"stay_id\", \"seq_num\"])\n",
    "\n",
    "print(f\"ED_Diag shape (length) BEFORE filtering rows with more than {max_seq_num} diagnosis: {ed_diagnosis.shape[0]}\")\n",
    "\n",
    "ed_diagnosis = ed_diagnosis.filter(\n",
    "    pl.col(\"seq_num\") < max_seq_num\n",
    ")\n",
    "print(f\"ED_Diag shape (length) AFTER filtering rows with more than {max_seq_num} diagnosis: {ed_diagnosis.shape[0]}\")\n",
    "\n",
    "\n",
    "ed_diagnosis_grouped = ed_diagnosis.group_by(\"stay_id\").agg(\n",
    "    pl.col(\"icd_title\").alias(\"icd_title_list\")\n",
    ")\n",
    "\n",
    "ed_diagnosis_grouped = ed_diagnosis_grouped.with_columns(\n",
    "                                                        pl.col(\"icd_title_list\")\n",
    "                                                        .map_elements(format_diagnoses, return_dtype=pl.Utf8)\n",
    "                                                        .alias(\"numbered_diagnoses\")\n",
    "                                                    ).drop(\"icd_title_list\")\n",
    "\n",
    "main_df = main_df.join(ed_diagnosis_grouped, on=\"stay_id\", how=\"inner\")\n",
    "\n",
    "print(f\"Shape of main_df BEFORE dropping non-alphanumeric ChiefComplaint: {main_df.shape[0]}\")\n",
    "main_df = main_df.filter(\n",
    "    pl.col(\"chiefcomplaint\").str.contains(r\"^[a-zA-Z0-9\\s,.!?;:'\\\"()-]+$\")\n",
    ")\n",
    "print(f\"Shape of main_df AFTER dropping non-alphanumeric ChiefComplaint: {main_df.shape[0]}\")\n",
    "\n",
    "main_df_copy = main_df.clone()\n",
    "\n",
    "main_df = main_df.filter(pl.col(\"pain\").is_in([i/10 for i in range(0, 105, 5)]))\n",
    "\n",
    "main_df.drop([\"hadm_id\", \"intime\", \"outtime\", \"dod\"])\n",
    "main_df = main_df.with_columns(\n",
    "    pl.when(pl.col(\"acuity\") < 3.0)\n",
    "    .then(pl.lit(\"YES\"))\n",
    "    .otherwise(pl.lit(\"NO\"))\n",
    "    .alias(\"GT_FLAG\")\n",
    ")\n",
    "main_df = main_df.join(admissions[[\"subject_id\", \"race\"]].unique(subset=\"subject_id\"), on=\"subject_id\", how=\"inner\")\n",
    "\n",
    "print()\n",
    "\n",
    "tdf = main_df[\"acuity\"].value_counts()\n",
    "\n",
    "rows_with_acuity_being_1_or_2 = tdf.filter(pl.col(\"acuity\").is_in([1.0, 2.0]))[\"count\"].sum()\n",
    "rows_with_acuity_being_3 = tdf.filter(pl.col(\"acuity\").is_in([3.0, 4.0, 5.0]))[\"count\"].sum()\n",
    "\n",
    "print(f\"Sum of rows with acuity 1.0 or 2.0: {rows_with_acuity_being_1_or_2}\")\n",
    "print(f\"Sum of rows with acuity 3.0: {rows_with_acuity_being_3}\")\n",
    "\n",
    "print()\n",
    "\n",
    "main_df = main_df.drop(\"acuity\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making PreFInal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length BEFORE dropping rows that are not present in the top 4 races: 223986\n",
      "Fractions BEFORE dropping rows that are not present in the top 4 races\n",
      "Fraction of NO: 0.57\n",
      "Fraction of YES: 0.43\n",
      "Length AFTER dropping rows that are not present in the top 4 races: 221575\n",
      "Fractions AFTER dropping rows that are not present in the top 4 races\n",
      "Fraction of NO: 0.57\n",
      "Fraction of YES: 0.43\n",
      "\n",
      "\n",
      "The shape of main_df currently is: (221575, 19)\n"
     ]
    }
   ],
   "source": [
    "main_df = main_df.with_columns(\n",
    "    pl.col(\"race\").str.split(\"-\").list.first().str.strip_chars().str.split(\"/\").list.first().str.strip_chars().alias(\"race\")\n",
    ")\n",
    "\n",
    "# Drop races like \"Unknown\", \"Other\"\n",
    "main_df = main_df.filter(pl.col('race') != 'UNKNOWN')\n",
    "main_df = main_df.filter(pl.col('race') != 'OTHER')\n",
    "main_df = main_df.filter(pl.col('race') != 'UNABLE TO OBTAIN')\n",
    "main_df = main_df.filter(pl.col('race') != 'PATIENT DECLINED TO ANSWER')\n",
    "\n",
    "main_df = main_df.with_columns(\n",
    "    pl.col(\"race\").replace({\"HISPANIC\": \"HISPANIC/LATINO\", \"HISPANIC OR LATINO\": \"HISPANIC/LATINO\"})\n",
    ")\n",
    "\n",
    "race_values = list(main_df['race'].value_counts(sort=True).head(4)['race'])\n",
    "gender_values = list(main_df['gender'].unique())\n",
    "\n",
    "demographic_dict = {\n",
    "    'gender': gender_values,\n",
    "    'race': race_values\n",
    "}\n",
    "\n",
    "df = main_df\n",
    "print(f\"Length BEFORE dropping rows that are not present in the top 4 races: {df.shape[0]}\")\n",
    "print(\"Fractions BEFORE dropping rows that are not present in the top 4 races\")\n",
    "print(f'Fraction of NO: {len(df.filter(pl.col(\"GT_FLAG\") == \"NO\")) / len(df):.2f}')\n",
    "print(f'Fraction of YES: {len(df.filter(pl.col(\"GT_FLAG\") == \"YES\")) / len(df):.2f}')\n",
    "\n",
    "main_df = main_df.filter(pl.col('race').is_in(demographic_dict['race']))\n",
    "\n",
    "df = main_df\n",
    "print(f\"Length AFTER dropping rows that are not present in the top 4 races: {df.shape[0]}\")\n",
    "print(\"Fractions AFTER dropping rows that are not present in the top 4 races\")\n",
    "print(f'Fraction of NO: {len(df.filter(pl.col(\"GT_FLAG\") == \"NO\")) / len(df):.2f}')\n",
    "print(f'Fraction of YES: {len(df.filter(pl.col(\"GT_FLAG\") == \"YES\")) / len(df):.2f}')\n",
    "\n",
    "print(f\"\\n\\nThe shape of main_df currently is: {main_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value counts of the races before undersamplign is: shape: (4, 2)\n",
      "┌─────────────────┬────────┐\n",
      "│ race            ┆ count  │\n",
      "│ ---             ┆ ---    │\n",
      "│ str             ┆ u32    │\n",
      "╞═════════════════╪════════╡\n",
      "│ HISPANIC/LATINO ┆ 19529  │\n",
      "│ WHITE           ┆ 138808 │\n",
      "│ ASIAN           ┆ 7906   │\n",
      "│ BLACK           ┆ 55332  │\n",
      "└─────────────────┴────────┘\n",
      "Fractions BEFORE undersampling the dataset such that all races occur at uniform intervals is:\n",
      "Fraction of NO: 0.57\n",
      "Fraction of YES: 0.43\n",
      "\n",
      "\n",
      "The shape of main_df is now: (31624, 19)\n",
      "\n",
      "\n",
      "Fractions AFTER undersampling the dataset such that all races occur at uniform intervals is:\n",
      "Fraction of NO: 0.60\n",
      "Fraction of YES: 0.40\n"
     ]
    }
   ],
   "source": [
    "print(f\"The value counts of the races before undersamplign is: {main_df['race'].value_counts()}\")\n",
    "\n",
    "df = main_df\n",
    "print(\"Fractions BEFORE undersampling the dataset such that all races occur at uniform intervals is:\")\n",
    "print(f'Fraction of NO: {len(df.filter(pl.col(\"GT_FLAG\") == \"NO\")) / len(df):.2f}')\n",
    "print(f'Fraction of YES: {len(df.filter(pl.col(\"GT_FLAG\") == \"YES\")) / len(df):.2f}')\n",
    "\n",
    "# Under sample the dataframe and have the races occuring at uniform intervals\n",
    "min_count = main_df['race'].value_counts(sort=True)[-1][0, 1]\n",
    "\n",
    "sampled_df = pl.DataFrame()\n",
    "\n",
    "for race in demographic_dict['race']:\n",
    "    tmp_df = main_df.filter(pl.col('race') == race)\n",
    "    if tmp_df.shape[0] > min_count:\n",
    "        tmp_df = tmp_df.sample(n = min_count, with_replacement=False, seed=42)\n",
    "    sampled_df = pl.concat([sampled_df, tmp_df])\n",
    "\n",
    "main_df_sampled = sampled_df\n",
    "print(f'\\n\\nThe shape of main_df is now: {main_df_sampled.shape}\\n\\n')\n",
    "\n",
    "df = main_df_sampled\n",
    "print(\"Fractions AFTER undersampling the dataset such that all races occur at uniform intervals is:\")\n",
    "print(f'Fraction of NO: {len(df.filter(pl.col(\"GT_FLAG\") == \"NO\")) / len(df):.2f}')\n",
    "print(f'Fraction of YES: {len(df.filter(pl.col(\"GT_FLAG\") == \"YES\")) / len(df):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant rows from the final dataframe to make the pre_final_df (the dataframe using which, prompts will be made according to the prompt template)\n",
    "pre_final_df = main_df_sampled.select(\n",
    "    cs.by_name(\n",
    "        \"subject_id\",\n",
    "        \"stay_id\",\n",
    "        \"gender\",\n",
    "        \"anchor_age\",\n",
    "        \"race\",\n",
    "        \"temperature\",\n",
    "        \"heartrate\",\n",
    "        \"resprate\",\n",
    "        \"o2sat\",\n",
    "        \"sbp\",\n",
    "        \"dbp\",\n",
    "        \"pain\",\n",
    "        \"chiefcomplaint\",\n",
    "        \"numbered_diagnoses\",\n",
    "        \"GT_FLAG\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_final_df.write_csv(pre_final_df_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal_graph2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
